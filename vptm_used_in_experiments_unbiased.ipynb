{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanthongtan/ptm/blob/master/vptm_used_in_experiments_unbiased.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyPA6CdzeV-T"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdXnyoAPeWyP"
      },
      "outputs": [],
      "source": [
        "num_topic = 50\n",
        "dataset = 'kos'\n",
        "method = 'vptm'\n",
        "\n",
        "#vptm hyperparameters\n",
        "alpha_scalar = 0.5\n",
        "v= 0.05\n",
        "c = 0.1 * v\n",
        "prior_mu = 'pos' #possible options: neg, pos, mean\n",
        "\n",
        "#GMC hyperparameters\n",
        "num_samples = 1\n",
        "num_burn = 65000\n",
        "S = 68000\n",
        "L = 1\n",
        "decay_factor = 1/3\n",
        "eta_theta = 1e-1\n",
        "rho_theta = 1e-1\n",
        "eta_mu = 5e-4\n",
        "rho_mu = 1e-1\n",
        "eta_kappa = 5e-1\n",
        "rho_kappa = 1e-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TITNtPmMho4H"
      },
      "source": [
        "# Run GMC Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsjM1AJO6KOm",
        "outputId": "8beabb48-1232-4308-84e1-9173308eba2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 31 13:05:35 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    45W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Cloning into 'ptm'...\n",
            "remote: Enumerating objects: 378, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 378 (delta 49), reused 42 (delta 16), pack-reused 276\u001b[K\n",
            "Receiving objects: 100% (378/378), 257.60 MiB | 36.02 MiB/s, done.\n",
            "Resolving deltas: 100% (181/181), done.\n",
            "/content/ptm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "/content/ptm/dataset.py:13: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:605.)\n",
            "  ret = torch.sparse.FloatTensor(indices, values, size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dim Training Data (2916, 6906)\n",
            "Dim Test Data (514, 6906)\n"
          ]
        }
      ],
      "source": [
        "#only for google colab\n",
        "import sys\n",
        "import os\n",
        "if 'google.colab' in sys.modules:\n",
        "    #lets see what gpu we were given\n",
        "    !nvidia-smi\n",
        "    #get repository\n",
        "    !git clone https://github.com/tanthongtan/ptm.git\n",
        "    %cd '/content/ptm'\n",
        "    #get ref corp if doesn't exist\n",
        "    if not os.path.isdir('wiki_final'):\n",
        "        !unzip -q \"/content/drive/My Drive/wiki_final.zip\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from geodesic import GeodesicMonteCarlo\n",
        "from dataset import load_data, csr_to_torchsparse\n",
        "import geodesic as g\n",
        "import distributions as D\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.distributions as dist\n",
        "import numpy as np\n",
        "import time\n",
        "from utils import print_topics, get_topics, vmf_perplexity, clustering_metrics_20news, print_summary, get_invalid_topics\n",
        "\n",
        "#make all tensors cuda if available and double\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
        "    gpu = True\n",
        "else:\n",
        "    torch.set_default_tensor_type(torch.DoubleTensor)\n",
        "    gpu = False\n",
        "\n",
        "#Load Data\n",
        "data_tr, data_te, vocab, vocab_size, num_tr = load_data(use_tfidf = True, sublinear = False, normalize = True, dataset = dataset)\n",
        "tensor_te = csr_to_torchsparse(data_te, gpu)\n",
        "tensor_tr = csr_to_torchsparse(data_tr, gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9cnu5Fgahie"
      },
      "outputs": [],
      "source": [
        "%%capture cap\n",
        "\n",
        "#declare tensor hyperparameters\n",
        "alpha = torch.full((1,num_topic), alpha_scalar)\n",
        "if prior_mu == 'neg':\n",
        "    mu0 = F.normalize(torch.full((vocab_size,),-1.0),dim=-1)\n",
        "if prior_mu == 'pos':\n",
        "    mu0 = F.normalize(torch.full((vocab_size,),1.0),dim=-1)\n",
        "if prior_mu == 'mean':\n",
        "    mu0 = F.normalize(torch.sparse.sum(tensor_tr,dim=0).to_dense(),dim=-1)\n",
        "\n",
        "#randomly initialize model parameters\n",
        "theta = torch.randn(num_tr,num_topic-1)\n",
        "mu = F.normalize(torch.randn(num_topic, vocab_size) / (vocab_size ** 0.5) + mu0, p=2, dim=-1)\n",
        "kappa = torch.randn(num_topic,1)*50 + 50. * (vocab_size ** 0.5)\n",
        "\n",
        "#declare GMC transition kernels\n",
        "kernel = GeodesicMonteCarlo(L)\n",
        "params = {'theta': theta, 'mu':mu, 'kappa': kappa}\n",
        "init_etas = {'theta': eta_theta, 'mu':eta_mu, 'kappa': eta_kappa}\n",
        "geodesics = {'theta': g.RnGeodesic(eta = eta_theta, rho = rho_theta), 'mu': g.SphericalGeodesic(eta = eta_mu, rho = rho_mu), 'kappa': g.PositiveGeodesic(eta = eta_kappa, rho = rho_kappa)}\n",
        "vs = {name: geodesics[name].projection(params[name],dist.MultivariateNormal(torch.zeros(params[name].shape[-1]), torch.eye(params[name].shape[-1])).sample([params[name].shape[0]])) for name in params}\n",
        "\n",
        "#start sampling loop\n",
        "t = tqdm(range(num_samples+num_burn))\n",
        "theta_samples = 0\n",
        "mu_samples = 0\n",
        "kappa_samples = 0\n",
        "mu_save_collection_samples = [] #this is to test for coherence by time\n",
        "kappa_save_collection_samples = [] #in case we need to test perp or something\n",
        "start_time = time.time() #get start time\n",
        "sampling_its = {50, 150, 450, 1350, 4050, 12150, 36450, 65000}\n",
        "#basically get torchsparse and check if stochastic gradient or not\n",
        "idx = torch.randperm(num_tr)[:S]\n",
        "x_batch = csr_to_torchsparse(data_tr[idx.cpu()], gpu)\n",
        "stochastic_gradient = False\n",
        "if idx.shape[0] < num_tr:\n",
        "    stochastic_gradient = True\n",
        "for i in t:\n",
        "\n",
        "    #we only need to draw random sample if stochastic gradient, save time for full gradient\n",
        "    if stochastic_gradient:\n",
        "        idx = torch.randperm(num_tr)[:S]\n",
        "        x_batch = csr_to_torchsparse(data_tr[idx.cpu()], gpu)\n",
        "\n",
        "    for name in geodesics:\n",
        "        geodesics[name].eta = init_etas[name] * ((i+1) ** (-(decay_factor)))\n",
        "    params, vs = kernel.stochastic_transition(params, vs, geodesics, D.VptmJointDistributionWithStickDirConjugatePriorUnbiased(x_batch, alpha, c, mu0, v, idx))\n",
        "\n",
        "    theta = params['theta']\n",
        "    kappa = params['kappa']\n",
        "    mu = params['mu']\n",
        "\n",
        "    if torch.any(kappa != kappa):\n",
        "        break\n",
        "\n",
        "    if i >= num_burn:\n",
        "        theta_samples += theta\n",
        "        mu_samples += mu\n",
        "        kappa_samples += kappa\n",
        "\n",
        "    if i % 1300 == 0 or i in sampling_its:\n",
        "        print(\"\\ncurrent iteration:\", i)\n",
        "        print(\"elapsed time\", time.time() - start_time)\n",
        "        print(\"kappa mean\",kappa.mean())\n",
        "        print(\"kappas\",torch.flatten(kappa))\n",
        "        pi = dist.StickBreakingTransform()(theta)\n",
        "        print(\"mu norms\", mu.norm(dim=-1).sum(), num_topic)\n",
        "        print(\"sparsity\",(torch.abs(mu)**2.).norm(dim=-1))\n",
        "        print(\"sparsitymean\",(torch.abs(mu)**2.).norm(dim=-1).mean())\n",
        "        print(\"pi sums\", pi.sum(dim=-1).sum(), num_tr)\n",
        "\n",
        "        sum_ll = 0.0\n",
        "        sum_cs = 0.0\n",
        "        for j in range(int(np.ceil(num_tr/S))):\n",
        "            curr_pi = pi[j*S:j*S+S]\n",
        "            curr_tensor_tr = csr_to_torchsparse(data_tr[j*S:j*S+S], gpu)\n",
        "            curr_avg = torch.matmul(curr_pi,kappa*mu)\n",
        "            sum_ll += D.log_prob_von_mises_fisher(curr_avg, curr_tensor_tr).sum()\n",
        "            curr_avg = F.normalize(curr_avg,dim=-1)\n",
        "            sum_cs += D.sparse_dense_dot(curr_tensor_tr, curr_avg).sum()\n",
        "\n",
        "        print(\"log likelihood\", sum_ll / num_tr)\n",
        "        print(\"cosine similarity\", sum_cs / num_tr)\n",
        "        print(\"perplexity\", vmf_perplexity(tensor_te, mu, kappa, alpha, N=1000))\n",
        "\n",
        "        sum_cs_spread = 0\n",
        "        count_cs = 0\n",
        "        for j in range(num_topic-1):\n",
        "            for k in range(j+1,num_topic):\n",
        "                sum_cs_spread += (mu[j] * mu[k]).sum(dim=-1)\n",
        "                count_cs += 1\n",
        "        print(\"mean cs spread\", sum_cs_spread / count_cs,\"\\n\")\n",
        "\n",
        "        pi_cpu = pi.cpu().numpy()\n",
        "        kappa_cpu = kappa.cpu().numpy()\n",
        "        print(\"invalid topics\")\n",
        "        print(\"normal thres\", get_invalid_topics(pi_cpu, kappa_cpu))\n",
        "        print(\"thres 2x\", get_invalid_topics(pi_cpu, kappa_cpu, 1/(num_topic*2)))\n",
        "        print(\"thres 10x\", get_invalid_topics(pi_cpu, kappa_cpu, 1/(num_topic*10)))\n",
        "        print()\n",
        "\n",
        "\n",
        "    if i % 13000 == 0:\n",
        "        emb = mu.cpu().numpy()\n",
        "        print_topics(get_topics(emb,vocab))\n",
        "        print(\"\")\n",
        "\n",
        "    if i in sampling_its:\n",
        "        mu_save_collection_samples.append(mu.cpu().numpy())\n",
        "        kappa_save_collection_samples.append(kappa.cpu().numpy())\n",
        "\n",
        "#get topic coherence\n",
        "mu_final = mu_samples / num_samples\n",
        "kappa_final = kappa_samples / num_samples\n",
        "theta_final = theta_samples / num_samples\n",
        "print('\\nalpha = ', alpha_scalar)\n",
        "print(\"final perplexity\", vmf_perplexity(tensor_te, mu_final, kappa_final, alpha, N=1000))\n",
        "print('prior_mu:', prior_mu)\n",
        "print('v:', v)\n",
        "print('c:', c)\n",
        "print('decay_factor:', decay_factor)\n",
        "emb = mu_final.cpu().numpy()\n",
        "topics = get_topics(emb, vocab)\n",
        "print_summary(topics,method,dataset)\n",
        "\n",
        "if dataset == '20news':\n",
        "    pi = dist.StickBreakingTransform()(theta_final)\n",
        "    pi = pi.cpu().numpy()\n",
        "    clustering_metrics_20news(pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VNycNx3p6jE"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "run = random.randint(0,100000)\n",
        "current_run_name = \"method=\"+str(method)+\", alpha=\"+str(alpha_scalar)+\", K=\"+str(num_topic)+\", dataset=\"+str(dataset)+ \", prior_mu=\"+str(prior_mu)+ \", v=\"+str(v)+ \", c=\"+str(c)+ \", decay_factor=\"+str(decay_factor)+ \", run=\"+ str(run)\n",
        "dir_first_part = str(v) + \"/\" + str(num_topic) + \"/\" + str(alpha_scalar) + \"/\" + str(dataset) + \"/\" + str(prior_mu) + \"/\" + str(c) + \"/\"\n",
        "dirname = '/content/drive/My Drive/masters_results_unbiased/'+ dir_first_part + current_run_name + \"/\"\n",
        "os.makedirs(dirname, exist_ok=False)\n",
        "filename = current_run_name +\".txt\"\n",
        "with open(dirname + filename, 'w') as f:\n",
        "     f.write(cap.stdout)\n",
        "\n",
        "#save mus\n",
        "emb_filename = current_run_name +\" emb.npy\"\n",
        "\n",
        "with open(dirname +emb_filename, 'wb') as f:\n",
        "    np.save(f, emb)\n",
        "\n",
        "#save kappas\n",
        "kappa_filename = current_run_name +\" kappa.npy\"\n",
        "\n",
        "with open(dirname +kappa_filename, 'wb') as f:\n",
        "    np.save(f, kappa_final.cpu().numpy())\n",
        "\n",
        "#save means\n",
        "mean_filename = current_run_name +\" mean.npy\"\n",
        "\n",
        "with open(dirname + mean_filename, 'wb') as f:\n",
        "    np.save(f, mu0.cpu().numpy())\n",
        "\n",
        "#save pis\n",
        "pi = dist.StickBreakingTransform()(theta_final)\n",
        "pi = pi.cpu().numpy()\n",
        "\n",
        "pi_filename = current_run_name + \" pi.npy\"\n",
        "\n",
        "with open(dirname + pi_filename, 'wb') as f:\n",
        "    np.save(f, pi)\n",
        "\n",
        "#save sample collections\n",
        "mu_collection_filename = current_run_name + \" mucollection.npz\"\n",
        "with open(dirname + mu_collection_filename, 'wb') as f:\n",
        "    np.savez_compressed(f, *mu_save_collection_samples)\n",
        "\n",
        "kappa_collection_filename = current_run_name + \" kappacollection.npz\"\n",
        "with open(dirname + kappa_collection_filename, 'wb') as f:\n",
        "    np.savez_compressed(f, *kappa_save_collection_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNJ-Ju0_WUFi"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1KxGH2_04Z8eeYTcnuMuQ_peryxW-EukR",
      "authorship_tag": "ABX9TyOV4gd2y+ASndjj+GmUPpd+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}